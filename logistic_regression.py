
# -*- coding: utf-8 -*-
"""cse417_hw3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wUDDmeU2Gihdw5HgQ2Ec3vyM4NwbTMvo
"""

import numpy as np
import csv 
import pandas as pd
import time
from scipy import stats

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

!ls '/content/gdrive/My Drive/'

def find_test_error(w, X, y):

  # Your code here, assign the proper value to test_error:
  n,m = X.shape
  total_success = np.sum(np.sign(np.dot(X, w)) == y)  #an integer that shows the total correctly classifies points
  test_error = (n-total_success)/n
  print(test_error)
  return test_error

def logistic_reg(X, y, w_init, max_its, eta):

  # Your code here, assign the proper values to t, w, and e_in:
  w = w_init                #w is 14*1
  gradient_numerator = y * X #y:152,1 ; Xnew : 152,14, element wise multiplication, broadcasting, so ans is 152 * 14
  for i in range(max_its):
    t = 1
    product = np.matmul(X, w)  #???why is 'product' of dimension 152*14 ??
    gradient_denom = 1 + np.exp(y * np.matmul(X, w))  #Xnew :152,14    w: 14,1  ans=152,1 ; y = 152,1, ans is 152*1
    gradient = np.divide(gradient_numerator, gradient_denom) #broadcasting happening here! ans dim = 152*14
    gradient = np.sum(gradient, axis = 0) * (1/n)  #the ans here would be 1*14
    w = w + eta * gradient.reshape(-1,1)  #w is 14*1
    if(any(np.absolute(gradient)) < pow(10,-3)):  #any means union
      break
    t = t + 1
  e_in = np.log(1+np.exp(-1 * y * np.matmul(X, w)))   #ans is 152*1 here
  e_in  = np.sum(e_in, axis = 0) *(1/n)               #ans is 1*1, sum over col
  print("e_in: "+str(e_in))
  return t, w, e_in

#with open('/content/gdrive/My Drive/cleveland_train.csv', 'r') as csvfile:
 
# reader = csv.reader(csvfile)
# tables = [table for table in reader]
#print(tables)
table = pd.read_csv('/content/gdrive/My Drive/cleveland_train.csv')
print(table)

table2 = pd.read_csv('/content/gdrive/My Drive/cleveland_test.csv')
print(table2)

#first part of the question where we have to get e_in for the basic logistic regression
y = np.array(table.iloc[:, -1])  #-1 means the last column
y[y==0] = -1
y = y.reshape(-1,1)  #converting y into a column vector
X = np.array(table.iloc[:,:-1])   # except for last column
n,m = X.shape  
X0 = np.ones((n,1))
Xnew = np.hstack((X0,X)) 

w_init = np.zeros((14,1))
max_its = pow(10, 6)
eta = pow(10, -5)
tic = time.clock()
print(X.shape)
print(Xnew.shape)
print(w_init.shape)
[t, w, e_in] = logistic_reg(Xnew, y, w_init, max_its, eta)
toc = time.clock()
print("time taken: "+ str(toc-tic))
test_error = find_test_error(w, Xnew, y)
print("training error:", test_error)
y = np.array(table2.iloc[:, -1])  #-1 means the last column
y[y==0] = -1
y = y.reshape(-1,1)  #converting y into a column vector
X = np.array(table2.iloc[:,:-1])   # except for last column
n,m = X.shape  
X0 = np.ones((n,1))
Xnew = np.hstack((X0,X)) 
test_error = find_test_error(w, Xnew, y)  #Xnew: 145*14
print("test error: ", test_error)

#normalization
# Xnew = np.hstack((X0,X)) 
y = np.array(table.iloc[:, -1])  #-1 means the last column
y[y==0] = -1
y = y.reshape(-1,1)  #converting y into a column vector
X = np.array(table.iloc[:,:-1])   # except for last column
X_mean = X.mean(0).reshape(1,13)
X_std = X.std(0).reshape(1,13)
print(X_mean.shape)
print(X_std.shape)
X = stats.zscore(X)
n,m = X.shape  
print(X.shape)
X0 = np.ones((n,1))
Xnew = np.hstack((X0,X)) 
print(Xnew.shape)

w_init = np.zeros((14,1))
max_its = pow(10, 10)
eta = 7.65
tic = time.clock()
[t, w, e_in] = logistic_reg_normalization_case(Xnew, y, w_init, max_its, eta)
toc = time.clock()
print("time taken: "+ str(toc-tic))
print("e_in", e_in)
print("number_of_iterations:", t)
test_error = find_test_error(w, Xnew, y)
print("training error: ", test_error)

y = np.array(table2.iloc[:, -1])  #-1 means the last column
y[y==0] = -1
y = y.reshape(-1,1)  #converting y into a column vector
X = np.array(table2.iloc[:,:-1])   # except for last column
print(X.shape)
print(X_mean.shape)
print(X_std.shape)
Z = (X - X_mean)/(X_std)
print(Z.shape)
# compute the mean of the train_x and the std of the train_x
# compute the z score with the train_x mean and the std. (x - mu)/std
n,m = Z.shape  
X0 = np.ones((n,1))
Xnew = np.hstack((X0,Z))

test_error = find_test_error(w, Xnew, y)  #Xnew: 145*14
print("test error: ", test_error)

def logistic_reg_normalization_case(X, y, w_init, max_its, eta):
  # Your code here, assign the proper values to t, w, and e_in:

  w = w_init                #w is 14*1
  t = 0
  gradient_numerator = y * X  #y:152,1 ; Xnew : 152,14, element wise multiplication, broadcasting, so ans is 152 * 14
  while True:
    t = t + 1
    product = np.matmul(X, w)  
    gradient_denom = 1 + np.exp(y * np.matmul(X, w))  #Xnew :152,14    w: 14,1  ans=152,1 ; y = 152,1, ans is 152*1
    gradient = np.divide(gradient_numerator, gradient_denom) #broadcasting happening here! ans dim = 152*14
    gradient = np.sum(gradient, axis = 0) * (1/n)  #the ans here would be 1*14
    w = w + eta * gradient.reshape(-1,1)  #w is 14*1
    #print(w.shape)
    if(all(np.absolute(gradient) < pow(10,-6))):  #all means taking intersection
      break
  e_in = np.log(1+np.exp(-1 * y * np.matmul(X, w)))   #ans is 152*1 here
  e_in  = np.sum(e_in, axis = 0) *(1/n)               #ans is 1*1, sum over col
  print("e_in: "+str(e_in))
  return t, w, e_in
STUDENT
Will Wang
AUTOGRADER SCORE
0.0 / 0.0
