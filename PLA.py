"""Will Wang(464460)hw1_code (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vfcoIozAFruZlGmtox6LIP18wZM6QLAg

# Homework 1 Code
"""

# Add import statements here
import numpy as np

"""## Perceptron Learning

The `perceptron_learn` function runs the Perceptron Learning Algorithm on input data.

Inputs: 
* `data_in` is a matrix with each row representing an $(x, y)$ pair; the $x$ vector is augmented with a leading 1, the label, $y$, is in the last column. 
Outputs:
The function outputs a tuple, `(w, iterations)`, where: 
* `w` is the learned weight vector; it should linearly separate the data if it is linearly separable.
* `iterations` is the number of iterations the algorithm ran for.
"""

def perceptron_learn(data_in):
  """
  1  x11      x12 ...    x1.10        y1                        w1
  1  x21      x22 ....   x2.10        y2                        w2
  ..    .. .                                                    ..
  1  x100.1  x100.2    x100.10       y100                       w11
  """
  
  # Your code here, assign the proper values to w and iterations: 

  row = len(data_in)
  col = len(data_in[0]) 
  w = np.zeros(col - 1)               
  x = data_in[:, 0:-1]                                     
  y = data_in[:, -1]                          
  iteration = 0
  

  while True:
    isMisclassifiedData = False
    iteration += 1
    for i in range(row):
      y_train = np.sign(np.dot(np.transpose(w), x[i]))     
      if y_train != y[i]:
        w = w + y[i]*x[i]
        isMisclassifiedData = True 
        break
    if isMisclassifiedData == False:
        break

  rho = y[0]*np.dot(w.T, x[0])
  R = np.linalg.norm(x[0])  
  for k in range(row):
    rho_this_iter = y[k]*np.dot(np.transpose(w), x[k])
    R_this_iter = np.linalg.norm(x[k])
    rho = min(rho_this_iter, rho)
    R = max(R_this_iter, R)    

  return w, iteration, R, rho

perceptron_learn(generateData(100, 10, generate_w_optimal(100,10)))

"""# Perceptron Experiment
Code for running the perceptron experiment in HW1. 

Inputs: 
* `N` is the number of training examples
* `d` is the dimensionality of each example (before adding the 1)
* `num_samples` is the number of times to repeat the experiment

Outputs:
* `num_iters` is the number of iterations the Perceptron Learning Algorithm takes for each sample
* `bound_minus_ni` is the difference between the theoretical bound and the actual number of iterations

Both outputs should be `num_samples` long.
"""

def perceptron_experiment(N, d, num_samples):
  
  # Your code here, assign the values to num_ters and bounds_minus_ni:

  iteration_array = []
  R_array = []
  w_array = []
  rho_array = []
  theoretical_bound = []
  bounds_minus_ni_array = []
  for i in range(num_samples):
    
    w_optimal = generate_w_optimal(N,d) 
    data = generateData(N, d, w_optimal)
    w, iterations, R, rho = perceptron_learn(data) 
    iteration_array.append(iterations) 
    R_array.append(R)
    w_array.append(w)
    rho_array.append(rho)
    theoretical_bound.append((R_array[i]**2 * (np.linalg.norm(w_array[i]))**2 )/ rho_array[i] **2)  
    bounds_minus_ni_array.append(abs(iteration_array[i] - theoretical_bound[i]))
  num_iters = iteration_array
  return num_iters, bounds_minus_ni_array

perceptron_experiment(100,10,1000)

def generateData(N, d, w_optimal):
  matrix = np.random.uniform(-1,1,(N,d+2))
  for i in range(N):
    matrix[i][0] = 1
    matrix[i][d+1] = np.sign(np.dot(matrix[i][0 : d+1], w_optimal))
  return matrix

def generate_w_optimal(N,d):
   w_optimal = np.random.uniform(0,1, d+1)
   w_optimal[0]= 0
   return w_optimal

generateData(100,10, generate_w_optimal(100,10))

"""## Run and Plot

Run the code below
"""

import matplotlib.pyplot as plt
import math
num_iters, bounds_minus_ni = perceptron_experiment(100, 10, 1000)
fig, ax = plt.subplots(figsize =(10, 7))
ax.hist(num_iters , bins = 50) 
ax.set_ylabel('frequency of occuring')
ax.set_xlabel('number of iterations')


fig1, ax1 = plt.subplots(figsize =(10, 7))
ax1.hist(np.log(bounds_minus_ni) , bins = 50) 
ax1.set_ylabel('frequency of occuring')
ax1.set_xlabel('log of difference between bounds and iteration numbers')
plt.show()
